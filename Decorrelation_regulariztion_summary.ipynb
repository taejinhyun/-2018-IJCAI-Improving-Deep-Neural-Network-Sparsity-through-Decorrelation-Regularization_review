{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Deep Neural Network Sparsity through Decorrelation Regularization\n",
    "\n",
    "Decorrelation(비상관성) 정규화를 통한 희소 DNN 개발 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract \n",
    "\n",
    " Group Lasso in most effective model compression methods since it generates structured sparse network. But it still has substaintial filter correlation among the convolution filters. So we propose to Decorrelation regularization with Group Lasso, which explicitly forces the network to learn a set of less correlated filters. So it effect in increase sparsity of the resulting model and leads to better compressing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    " Well trained sparse network with group Lasso regularization still exist significant correlation among the filters. It means that heter is still room for further compression. \n",
    " So we propose the filter decorrelation regularizzation for reducing the filter correlation and improving the network sparsity. During the training process, a sparse mask is maintained for each layer, and we minimize the correlation of the filters which have not been masked. It explicitly forces the network to learn a set of less correlated filters, and the sparse mask ensures that decorrelation regularization is only imposed to those indispensable filters. \n",
    " \n",
    " Experimental results show that by using our decorrelaion regularization, the filter correlation could be effectively weakened than comparison other  network compression methods and using the sparsity regularization solely.\n",
    " \n",
    " ![fig1](fig1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Works \n",
    " 1. Network Quantization \n",
    " \n",
    "  Got efficient of acceleration by XNOR operations.\n",
    "  \n",
    "  \n",
    " 2. Weight Pruning and Decomposition\n",
    " \n",
    "  APOZ pruning, Entropy based pruning, CP decomposition, Tucker decomposition\n",
    "  \n",
    "  \n",
    " 3. Sparsity Rgularization \n",
    " \n",
    "  Group Lasso achieves high compression rate than pruning a pre-trained network, by parameters in some groups maked all zero safely. In addition Exclusive Lasso claimed by Yoon, it forces each filter to use different inputs, thus the hidden layers will learn more meaningful features.\n",
    "  \n",
    "  \n",
    " 4. Specially Designed Architectures\n",
    " \n",
    "  Cheap layer-wise convolution, point-wise convolution and zero FLOP shift layer were claimed, they say that using these alternative to the spatial convolution can achieve state-of-the-art performance with much less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    " 1. Motivation \n",
    "  \n",
    "  To measure the filter correlation, they define mean filter correlation $Corr_{l}$, C is Pearson correlation coefficient matrix, and $C_{i}$ is $i$th row of matrix C. $$Corr_{l}=\\frac{1}{N}\\sum^{N}_{i=1} max(|C_{i}-I_{i}|)$$ and whole network's mean filter correlation define as $$Corr=\\frac{1}{N}\\sum^{L}_{l=1}Corr_{l}$$\n",
    "  \n",
    "  So Doing validated on CIFAR 10 with VGG-16, there still remains a noticeable correlation within he remaining filters. It inspires them that removing filter correlation may be the key to the further network compression.\n",
    "  \n",
    "  \n",
    "  \n",
    "  2. Filter Decorrelation\n",
    "  \n",
    "   Instead of decomposing pretrained weights, we propose to minimize the filter correlation during traing, and learn a set of less correlated filters directly. To achieve this goal, it's natiral to extract the non-diagonal elements of filter correlation matrix, and add its l2 norm to the loss function, which will act as another regularizer and reduce the filter correlation directly.$$R^{l}_{C}=||C^{l}-I||^{2}_{2}$$\n",
    "   \n",
    "   However, due to the structured sparsity induced by group LASSO regularization, the number of valid filters is changing dynamically during the training process. The filters that have values all close to zero will become uncorrelated to other filter. Thus the loss value will be drastically reduced by these 'dying' filters. Another concern is that hte dead filters will be removed right after training, reducing the correlation between these unnecessarry filters does not benefits the final performance. \n",
    "   \n",
    "   Therefore they introduce a saprse mask according to the mean absolute values of each filter $$\\begin{cases} 1, & \\text{if} \\frac{1}{|W^{l}_{k}|} \\sum^{|W^{l}_{k}|}_{i=1}|(W^{l}_{k})_{i}|>t \\\\ 0, & \\text{otherwise} \\end{cases}$$\n",
    "   If mask has value 1, hen it will be sent to compute the correlation matrix, while 0, will be skipped in this iteration. $t$ is predefined threshold, and we use mean absolute values to prevent the undesired weighting effect caused by the different number of filters in different layers. With the sparse mask, the improved filter decorrelation regularization term is formulated as $$R^{l}_{C}=\\frac{1}{\\sum_{k}M^{l}_{k}}||\\hat{C}^{l}-I||^{2}_{2}||$$\n",
    "   $\\hat{C}^{l}$ is the correlation matrix of the unmasked filters in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    " This paper's main idea is Decorrelation regularization by sparse mask. they propose filter decorrelation regularization for reducng the filter correlation and improving the network sparsity. \n",
    " First they investigated Group LASSO's filter correlation to CIFAR datase. Filter correlation computed mean filter correlation term is formulated as : $$Corr_{l}=\\frac{1}{N}\\sum^{N}_{i=1} max(|C_{i}-I_{i}|)$$ and whole network's mean filter correlation define as $$Corr=\\frac{1}{N}\\sum^{L}_{l=1}Corr_{l}$$ And they also use Pearson correlaiton coefficient matrix. As result there had still significant correlation among the convolution filters, even well trained model. It indicate that there is still room for futher compression. \n",
    " So to achieve their goal, they compute filter correlation matrix and extract non diagonal elements, and add its l2 norm to the loss function, which will act as another regularizer and reduce the filter correlation directly. $$R^{l}_{C}=||C^{l}-I||^{2}_{2}$$\n",
    " \n",
    " However, due to the structured sparsity induced by group LASSO regularization, the number of valid filters is changing dynamically during the training process. Thus they propose sparse mask term is formulated as : $$\\begin{cases} 1, & \\text{if} \\frac{1}{|W^{l}_{k}|} \\sum^{|W^{l}_{k}|}_{i=1}|(W^{l}_{k})_{i}|>t \\\\ 0, & \\text{otherwise} \\end{cases}$$\n",
    "   If mask has value 1, hen it will be sent to compute the correlation matrix, while 0, will be skipped in this iteration. $t$ is predefined threshold, and we use mean absolute values to prevent the undesired weighting effect caused by the different number of filters in different layers. With the sparse mask, the improved filter decorrelation regularization term is formulated as $$R^{l}_{C}=\\frac{1}{\\sum_{k}M^{l}_{k}}||\\hat{C}^{l}-I||^{2}_{2}||(\\hat{C}^{l}\\text{is the correlation matrix of the unmasked filters in layer $l$.})$$  Breifly unmasking filters in each layer has computed decorrelation regularization after computing correlation matrix to compress network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
